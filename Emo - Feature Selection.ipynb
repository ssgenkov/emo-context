{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.sparse import hstack\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from scipy import sparse\n",
    "import lightgbm\n",
    "from sklearn.model_selection import train_test_split \n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.feature_extraction.text  import CountVectorizer\n",
    "import starterkit\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "import os\n",
    "import io"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'starterkit' from 'D:\\\\Machine Learning\\\\Notebooks\\\\EmoContext\\\\starterkit.py'>"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from importlib import reload\n",
    "\n",
    "reload(starterkit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.read_csv('D:\\\\Machine Learning\\\\Datasets\\\\EmoContext\\\\train.txt',\n",
    "                        sep='\\t', index_col='id')\n",
    "X = train_data.iloc[:,:-1]\n",
    "y = train_data.iloc[:,-1]\n",
    "le = LabelEncoder()\n",
    "y = le.fit_transform(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_data = pd.read_csv('D:\\\\Machine Learning\\\\Datasets\\\\EmoContext\\\\dev.txt',\n",
    "                        sep='\\t', index_col='id')\n",
    "X_dev = dev_data.iloc[:,:-1]\n",
    "y_dev = dev_data.iloc[:,-1]\n",
    "y_dev = le.transform(y_dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lightGbmTestScore(X, y):\n",
    "    train_x, val_x, train_y, val_y = train_test_split(X,\n",
    "                                                  y,\n",
    "                                                 test_size=0.2,\n",
    "                                                 random_state=42)\n",
    "    clf = lightgbm.LGBMClassifier(random_state=42)\n",
    "    clf.fit(train_x, train_y)\n",
    "    val_pred = clf.predict(val_x)\n",
    "    return f1_score(y_true=val_y, y_pred=val_pred,average='micro' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lightGbmTraingScore(X, y):\n",
    "    clf = lightgbm.LGBMClassifier(random_state=42)\n",
    "    clf.fit(X, y)\n",
    "    pred = clf.predict(X)\n",
    "    return f1_score(y_true=y, y_pred=pred,average='micro' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lightGbmValidationKitScore(X, y):\n",
    "    train_x, val_x, train_y, val_y = train_test_split(X,\n",
    "                                                  y,\n",
    "                                                 test_size=0.2,\n",
    "                                                 random_state=42)\n",
    "    clf = lightgbm.LGBMClassifier(random_state=42)\n",
    "    clf.fit(train_x, train_y)\n",
    "    val_pred = clf.predict(val_x)\n",
    "    lb = LabelBinarizer()\n",
    "    ground = lb.fit_transform(val_y)\n",
    "    predictions = lb.transform(val_pred)\n",
    "    return  starterkit.getMetrics(predictions, ground)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lightGbmTrainDevKitScore(X_train, y_train, X_dev, y_dev, showTrainingScore=False):\n",
    "    return trainDevKitScore(lightgbm.LGBMClassifier(random_state=42),X_train, y_train,\n",
    "                            X_dev, y_dev,showTrainingScore)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainDevKitScore(clf, X_train, y_train, X_dev, y_dev, showTrainingScore=False):\n",
    "    clf.fit(X_train, y_train)\n",
    "    \n",
    "    lb = LabelBinarizer()\n",
    "    lb.fit(y_train)\n",
    "    \n",
    "    if showTrainingScore:\n",
    "        print('Training score \\n')\n",
    "        train_pred = clf.predict(X_train)\n",
    "        ground = lb.transform(y_train)\n",
    "        predictions = lb.transform(train_pred)\n",
    "        starterkit.getMetrics(predictions=predictions, ground=ground)\n",
    "    \n",
    "    print('\\nDev score \\n')\n",
    "    dev_pred = clf.predict(X_dev)\n",
    "    ground = lb.transform(y_dev)\n",
    "    predictions = lb.transform(dev_pred)\n",
    "    return  starterkit.getMetrics(predictions=predictions, ground=ground)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lightGbmTrainingKitScore(X, y):\n",
    "    clf = lightgbm.LGBMClassifier()\n",
    "    clf.fit(X, y)\n",
    "    pred = clf.predict(X)\n",
    "    lb = LabelBinarizer()\n",
    "    ground = lb.fit_transform(y)\n",
    "    predictions = lb.transform(pred)\n",
    "    return  starterkit.getMetrics(predictions, ground)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_feats(feats_dir, folder):\n",
    "    dict_to_feed = {}\n",
    "    full_path = os.path.join(feats_dir, folder)\n",
    "    files = [f for f in os.listdir(full_path) \n",
    "             if os.path.isfile(os.path.join(full_path, f)) and f[-4:] =='.npz']\n",
    "    \n",
    "    for f in files:\n",
    "        dict_to_feed[f[:-4]] = sparse.load_npz(os.path.join(full_path, f))\n",
    "    return dict_to_feed    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [],
   "source": [
    "feats_path = 'D:\\\\Machine Learning\\\\Datasets\\\\EmoContext\\\\Features'\n",
    "\n",
    "train_feats_dict = load_feats(feats_path, 'train')\n",
    "dev_feats_dict = load_feats(feats_path, 'dev')\n",
    "test_feats_dict = load_feats(feats_path, 'test')\n",
    "\n",
    "train_feats = hstack(list(train_feats_dict.values()))\n",
    "dev_feats = hstack(list(dev_feats_dict.values()))\n",
    "test_feats = hstack(list(test_feats_dict.values()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing with some basic combination of features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Basic features only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training score \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Simeon\\Anaconda3\\envs\\First\\lib\\site-packages\\lightgbm\\basic.py:452: UserWarning: Converting data to scipy sparse matrix.\n",
      "  warnings.warn('Converting data to scipy sparse matrix.')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True Positives per class :  [ 4195.  3213. 13790.  4194.]\n",
      "False Positives per class :  [ 878.  439. 2795.  656.]\n",
      "False Negatives per class :  [1311. 1030. 1158. 1269.]\n",
      "Class happy : Precision : 0.880, Recall : 0.757, F1 : 0.814\n",
      "Class sad : Precision : 0.831, Recall : 0.923, F1 : 0.875\n",
      "Class angry : Precision : 0.865, Recall : 0.768, F1 : 0.813\n",
      "Ignoring the Others class, Macro Precision : 0.8587, Macro Recall : 0.8158, Macro F1 : 0.8367\n",
      "Ignoring the Others class, Micro TP : 21197, FP : 3890, FN : 3457\n",
      "Accuracy : 0.8419, Micro Precision : 0.8449, Micro Recall : 0.8598, Micro F1 : 0.8523\n",
      "\n",
      "Dev score \n",
      "\n",
      "True Positives per class :  [ 105.   87. 2095.   76.]\n",
      "False Positives per class :  [143.  76. 112.  61.]\n",
      "False Negatives per class :  [ 45.  55. 243.  49.]\n",
      "Class happy : Precision : 0.534, Recall : 0.613, F1 : 0.570\n",
      "Class sad : Precision : 0.949, Recall : 0.896, F1 : 0.922\n",
      "Class angry : Precision : 0.555, Recall : 0.608, F1 : 0.580\n",
      "Ignoring the Others class, Macro Precision : 0.6792, Macro Recall : 0.7056, Macro F1 : 0.6922\n",
      "Ignoring the Others class, Micro TP : 2258, FP : 249, FN : 347\n",
      "Accuracy : 0.8577, Micro Precision : 0.9007, Micro Recall : 0.8668, Micro F1 : 0.8834\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Simeon\\Anaconda3\\envs\\First\\lib\\site-packages\\lightgbm\\basic.py:452: UserWarning: Converting data to scipy sparse matrix.\n",
      "  warnings.warn('Converting data to scipy sparse matrix.')\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.8577132486388385,\n",
       " 0.9006781013163143,\n",
       " 0.8667946257197697,\n",
       " 0.8834115805946793)"
      ]
     },
     "execution_count": 237,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lightGbmTrainDevKitScore(train_feats, y, dev_feats, y_dev, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Lexicon only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training score \n",
      "\n",
      "True Positives per class :  [ 3316.  2244. 13864.  2575.]\n",
      "False Positives per class :  [ 773.  596. 6248.  544.]\n",
      "False Negatives per class :  [2190. 1999. 1084. 2888.]\n",
      "Class happy : Precision : 0.790, Recall : 0.529, F1 : 0.634\n",
      "Class sad : Precision : 0.689, Recall : 0.927, F1 : 0.791\n",
      "Class angry : Precision : 0.826, Recall : 0.471, F1 : 0.600\n",
      "Ignoring the Others class, Macro Precision : 0.7684, Macro Recall : 0.6426, Macro F1 : 0.6999\n",
      "Ignoring the Others class, Micro TP : 18683, FP : 7388, FN : 5971\n",
      "Accuracy : 0.7294, Micro Precision : 0.7166, Micro Recall : 0.7578, Micro F1 : 0.7366\n",
      "\n",
      "Dev score \n",
      "\n",
      "True Positives per class :  [  83.   50. 2047.   51.]\n",
      "False Positives per class :  [121. 104. 208.  91.]\n",
      "False Negatives per class :  [ 67.  92. 291.  74.]\n",
      "Class happy : Precision : 0.325, Recall : 0.352, F1 : 0.338\n",
      "Class sad : Precision : 0.908, Recall : 0.876, F1 : 0.891\n",
      "Class angry : Precision : 0.359, Recall : 0.408, F1 : 0.382\n",
      "Ignoring the Others class, Macro Precision : 0.5305, Macro Recall : 0.5452, Macro F1 : 0.5378\n",
      "Ignoring the Others class, Micro TP : 2148, FP : 403, FN : 457\n",
      "Accuracy : 0.8098, Micro Precision : 0.8420, Micro Recall : 0.8246, Micro F1 : 0.8332\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.8098003629764066,\n",
       " 0.8420227361818895,\n",
       " 0.8245681381957773,\n",
       " 0.8332040341349883)"
      ]
     },
     "execution_count": 240,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lightGbmTrainDevKitScore(train_feats_dict['dm_lexicon_feats'], y,\n",
    "                         dev_feats_dict['dm_lexicon_feats'], y_dev, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training score \n",
      "\n",
      "True Positives per class :  [ 2983.    79. 13565.   995.]\n",
      "False Positives per class :  [2871.   56. 8734.  877.]\n",
      "False Negatives per class :  [2523. 4164. 1383. 4468.]\n",
      "Class happy : Precision : 0.585, Recall : 0.019, F1 : 0.036\n",
      "Class sad : Precision : 0.608, Recall : 0.907, F1 : 0.728\n",
      "Class angry : Precision : 0.532, Recall : 0.182, F1 : 0.271\n",
      "Ignoring the Others class, Macro Precision : 0.5750, Macro Recall : 0.3694, Macro F1 : 0.4498\n",
      "Ignoring the Others class, Micro TP : 14639, FP : 9667, FN : 10015\n",
      "Accuracy : 0.5843, Micro Precision : 0.6023, Micro Recall : 0.5938, Micro F1 : 0.5980\n",
      "\n",
      "Dev score \n",
      "\n",
      "True Positives per class :  [7.100e+01 1.000e+00 2.122e+03 2.300e+01]\n",
      "False Positives per class :  [217.   6. 260.  55.]\n",
      "False Negatives per class :  [ 79. 141. 216. 102.]\n",
      "Class happy : Precision : 0.143, Recall : 0.007, F1 : 0.013\n",
      "Class sad : Precision : 0.891, Recall : 0.908, F1 : 0.899\n",
      "Class angry : Precision : 0.295, Recall : 0.184, F1 : 0.227\n",
      "Ignoring the Others class, Macro Precision : 0.4429, Macro Recall : 0.3662, Macro F1 : 0.4009\n",
      "Ignoring the Others class, Micro TP : 2146, FP : 321, FN : 459\n",
      "Accuracy : 0.8047, Micro Precision : 0.8699, Micro Recall : 0.8238, Micro F1 : 0.8462\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.8047186932849365,\n",
       " 0.8698824483177949,\n",
       " 0.8238003838771593,\n",
       " 0.8462145110410094)"
      ]
     },
     "execution_count": 242,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lightGbmTrainDevKitScore(train_feats_dict['nrc_lexicon_feats'], y,\n",
    "                         dev_feats_dict['nrc_lexicon_feats'], y_dev, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training score \n",
      "\n",
      "True Positives per class :  [ 3881.  2297. 13724.  2938.]\n",
      "False Positives per class :  [1038.  551. 5032.  699.]\n",
      "False Negatives per class :  [1625. 1946. 1224. 2525.]\n",
      "Class happy : Precision : 0.807, Recall : 0.541, F1 : 0.648\n",
      "Class sad : Precision : 0.732, Recall : 0.918, F1 : 0.814\n",
      "Class angry : Precision : 0.808, Recall : 0.538, F1 : 0.646\n",
      "Ignoring the Others class, Macro Precision : 0.7820, Macro Recall : 0.6658, Macro F1 : 0.7192\n",
      "Ignoring the Others class, Micro TP : 18959, FP : 6282, FN : 5695\n",
      "Accuracy : 0.7573, Micro Precision : 0.7511, Micro Recall : 0.7690, Micro F1 : 0.7600\n",
      "\n",
      "Dev score \n",
      "\n",
      "True Positives per class :  [  94.   54. 2050.   66.]\n",
      "False Positives per class :  [130. 106. 161.  94.]\n",
      "False Negatives per class :  [ 56.  88. 288.  59.]\n",
      "Class happy : Precision : 0.338, Recall : 0.380, F1 : 0.358\n",
      "Class sad : Precision : 0.927, Recall : 0.877, F1 : 0.901\n",
      "Class angry : Precision : 0.412, Recall : 0.528, F1 : 0.463\n",
      "Ignoring the Others class, Macro Precision : 0.5591, Macro Recall : 0.5950, Macro F1 : 0.5765\n",
      "Ignoring the Others class, Micro TP : 2170, FP : 361, FN : 435\n",
      "Accuracy : 0.8218, Micro Precision : 0.8574, Micro Recall : 0.8330, Micro F1 : 0.8450\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.8217785843920146,\n",
       " 0.8573686290003951,\n",
       " 0.8330134357005758,\n",
       " 0.8450155763239876)"
      ]
     },
     "execution_count": 243,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lightGbmTrainDevKitScore(hstack([train_feats_dict['nrc_lexicon_feats'],\n",
    "                                train_feats_dict['dm_lexicon_feats']]), y,\n",
    "                         hstack([dev_feats_dict['nrc_lexicon_feats'],\n",
    "                                 dev_feats_dict['dm_lexicon_feats']]), y_dev, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Emojis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<30160x196 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 7196 stored elements in COOrdinate format>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emoji_feats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training score \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Simeon\\Anaconda3\\envs\\First\\lib\\site-packages\\lightgbm\\basic.py:452: UserWarning: Converting data to scipy sparse matrix.\n",
      "  warnings.warn('Converting data to scipy sparse matrix.')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True Positives per class :  [  276.  1390. 14602.  1043.]\n",
      "False Positives per class :  [   92.   270. 12339.   148.]\n",
      "False Negatives per class :  [5230. 2853.  346. 4420.]\n",
      "Class happy : Precision : 0.837, Recall : 0.328, F1 : 0.471\n",
      "Class sad : Precision : 0.542, Recall : 0.977, F1 : 0.697\n",
      "Class angry : Precision : 0.876, Recall : 0.191, F1 : 0.313\n",
      "Ignoring the Others class, Macro Precision : 0.7517, Macro Recall : 0.4985, Macro F1 : 0.5994\n",
      "Ignoring the Others class, Micro TP : 17035, FP : 12757, FN : 7619\n",
      "Accuracy : 0.5740, Micro Precision : 0.5718, Micro Recall : 0.6910, Micro F1 : 0.6258\n",
      "\n",
      "Dev score \n",
      "\n",
      "True Positives per class :  [   9.   55. 2291.   15.]\n",
      "False Positives per class :  [  6.  31. 336.  12.]\n",
      "False Negatives per class :  [141.  87.  47. 110.]\n",
      "Class happy : Precision : 0.640, Recall : 0.387, F1 : 0.482\n",
      "Class sad : Precision : 0.872, Recall : 0.980, F1 : 0.923\n",
      "Class angry : Precision : 0.556, Recall : 0.120, F1 : 0.197\n",
      "Ignoring the Others class, Macro Precision : 0.6891, Macro Recall : 0.4957, Macro F1 : 0.5766\n",
      "Ignoring the Others class, Micro TP : 2361, FP : 379, FN : 244\n",
      "Accuracy : 0.8603, Micro Precision : 0.8617, Micro Recall : 0.9063, Micro F1 : 0.8834\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Simeon\\Anaconda3\\envs\\First\\lib\\site-packages\\lightgbm\\basic.py:452: UserWarning: Converting data to scipy sparse matrix.\n",
      "  warnings.warn('Converting data to scipy sparse matrix.')\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.8602540834845736,\n",
       " 0.8616788321167883,\n",
       " 0.9063339731285989,\n",
       " 0.8834424695977549)"
      ]
     },
     "execution_count": 245,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lightGbmTrainDevKitScore(train_feats_dict['emoji_feats'].astype(np.float64), y,\n",
    "                         dev_feats_dict['emoji_feats'].astype(np.float64), y_dev, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###Count_vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "ctv = CountVectorizer(analyzer='word',token_pattern=r'\\w{1,}',\n",
    "            ngram_range=(1, 3), stop_words = 'english')\n",
    "full_conv = X[['turn1', 'turn2', 'turn3']].apply(lambda x: ' '.join(x), axis=1)\n",
    "full_conv_CVect = ctv.fit_transform(full_conv)\n",
    "\n",
    "full_conv_dev = X_dev[['turn1', 'turn2', 'turn3']].apply(lambda x: ' '.join(x), axis=1)\n",
    "full_conv_CVect_dev = ctv.transform(full_conv_dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training score \n",
      "\n",
      "True Positives per class :  [ 4398.  2720. 13569.  3408.]\n",
      "False Positives per class :  [ 707.  786. 3935.  637.]\n",
      "False Negatives per class :  [1108. 1523. 1379. 2055.]\n",
      "Class happy : Precision : 0.776, Recall : 0.641, F1 : 0.702\n",
      "Class sad : Precision : 0.775, Recall : 0.908, F1 : 0.836\n",
      "Class angry : Precision : 0.843, Recall : 0.624, F1 : 0.717\n",
      "Ignoring the Others class, Macro Precision : 0.7978, Macro Recall : 0.7242, Macro F1 : 0.7592\n",
      "Ignoring the Others class, Micro TP : 19697, FP : 5358, FN : 4957\n",
      "Accuracy : 0.7989, Micro Precision : 0.7862, Micro Recall : 0.7989, Micro F1 : 0.7925\n",
      "\n",
      "Dev score \n",
      "\n",
      "True Positives per class :  [ 122.   74. 2067.   76.]\n",
      "False Positives per class :  [100. 117. 122.  77.]\n",
      "False Negatives per class :  [ 28.  68. 271.  49.]\n",
      "Class happy : Precision : 0.387, Recall : 0.521, F1 : 0.444\n",
      "Class sad : Precision : 0.944, Recall : 0.884, F1 : 0.913\n",
      "Class angry : Precision : 0.497, Recall : 0.608, F1 : 0.547\n",
      "Ignoring the Others class, Macro Precision : 0.6095, Macro Recall : 0.6711, Macro F1 : 0.6388\n",
      "Ignoring the Others class, Micro TP : 2217, FP : 316, FN : 388\n",
      "Accuracy : 0.8490, Micro Precision : 0.8752, Micro Recall : 0.8511, Micro F1 : 0.8630\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.8490018148820326, 0.875246742992499, 0.8510556621880998, 0.8629817049435579)"
      ]
     },
     "execution_count": 250,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lightGbmTrainDevKitScore(full_conv_CVect.astype(np.float64), y,\n",
    "                       full_conv_CVect_dev.astype(np.float64), y_dev, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Manual features + Count_vectors "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training score \n",
      "\n",
      "True Positives per class :  [ 4737.  3727. 14067.  4677.]\n",
      "False Positives per class :  [ 430.  457. 1649.  416.]\n",
      "False Negatives per class :  [769. 516. 881. 786.]\n",
      "Class happy : Precision : 0.891, Recall : 0.878, F1 : 0.885\n",
      "Class sad : Precision : 0.895, Recall : 0.941, F1 : 0.917\n",
      "Class angry : Precision : 0.918, Recall : 0.856, F1 : 0.886\n",
      "Ignoring the Others class, Macro Precision : 0.9014, Macro Recall : 0.8919, Macro F1 : 0.8966\n",
      "Ignoring the Others class, Micro TP : 22471, FP : 2522, FN : 2183\n",
      "Accuracy : 0.9021, Micro Precision : 0.8991, Micro Recall : 0.9115, Micro F1 : 0.9052\n",
      "\n",
      "Dev score \n",
      "\n",
      "True Positives per class :  [ 119.  108. 2153.   95.]\n",
      "False Positives per class :  [79. 81. 81. 39.]\n",
      "False Negatives per class :  [ 31.  34. 185.  30.]\n",
      "Class happy : Precision : 0.571, Recall : 0.761, F1 : 0.653\n",
      "Class sad : Precision : 0.964, Recall : 0.921, F1 : 0.942\n",
      "Class angry : Precision : 0.709, Recall : 0.760, F1 : 0.734\n",
      "Ignoring the Others class, Macro Precision : 0.7480, Macro Recall : 0.8138, Macro F1 : 0.7795\n",
      "Ignoring the Others class, Micro TP : 2356, FP : 201, FN : 249\n",
      "Accuracy : 0.8984, Micro Precision : 0.9214, Micro Recall : 0.9044, Micro F1 : 0.9128\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Simeon\\Anaconda3\\envs\\First\\lib\\site-packages\\lightgbm\\basic.py:452: UserWarning: Converting data to scipy sparse matrix.\n",
      "  warnings.warn('Converting data to scipy sparse matrix.')\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.8983666061705989, 0.9213922565506453, 0.9044145873320537, 0.912824486633088)"
      ]
     },
     "execution_count": 247,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lightGbmTrainDevKitScore(hstack([full_conv_CVect, train_feats]), y,\n",
    "                         hstack([full_conv_CVect_dev, dev_feats]), y_dev, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### using lemmatized train data for count vect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_lemmatized = pd.read_csv(features_path+r'\\X_repld_low_lemmatized.csv')\n",
    "dev_data_lemmatized = pd.read_csv(features_path+r'\\dev_repld_low_lemmatized.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [],
   "source": [
    "ctv_lem = CountVectorizer(analyzer='word',token_pattern=r'\\w{1,}',\n",
    "            ngram_range=(1, 3), stop_words = 'english')\n",
    "full_conv_lem = train_data_lemmatized[['turn1', 'turn2', 'turn3']].apply(lambda x: ' '.join(x), axis=1)\n",
    "full_conv_CVect_lem = ctv_lem.fit_transform(full_conv_lem)\n",
    "full_conv_lem_dev = dev_data_lemmatized[['turn1', 'turn2', 'turn3']].apply(lambda x: ' '.join(x), axis=1)\n",
    "full_conv_CVect_lem_dev = ctv_lem.transform(full_conv_lem_dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training score \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Simeon\\Anaconda3\\envs\\First\\lib\\site-packages\\lightgbm\\basic.py:452: UserWarning: Converting data to scipy sparse matrix.\n",
      "  warnings.warn('Converting data to scipy sparse matrix.')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True Positives per class :  [ 4746.  3729. 14036.  4680.]\n",
      "False Positives per class :  [ 446.  483. 1622.  418.]\n",
      "False Negatives per class :  [760. 514. 912. 783.]\n",
      "Class happy : Precision : 0.885, Recall : 0.879, F1 : 0.882\n",
      "Class sad : Precision : 0.896, Recall : 0.939, F1 : 0.917\n",
      "Class angry : Precision : 0.918, Recall : 0.857, F1 : 0.886\n",
      "Ignoring the Others class, Macro Precision : 0.8999, Macro Recall : 0.8915, Macro F1 : 0.8957\n",
      "Ignoring the Others class, Micro TP : 22445, FP : 2523, FN : 2209\n",
      "Accuracy : 0.9016, Micro Precision : 0.8990, Micro Recall : 0.9104, Micro F1 : 0.9046\n",
      "\n",
      "Dev score \n",
      "\n",
      "True Positives per class :  [ 118.  106. 2168.   95.]\n",
      "False Positives per class :  [66. 74. 84. 44.]\n",
      "False Negatives per class :  [ 32.  36. 170.  30.]\n",
      "Class happy : Precision : 0.589, Recall : 0.746, F1 : 0.658\n",
      "Class sad : Precision : 0.963, Recall : 0.927, F1 : 0.945\n",
      "Class angry : Precision : 0.683, Recall : 0.760, F1 : 0.720\n",
      "Ignoring the Others class, Macro Precision : 0.7450, Macro Recall : 0.8113, Macro F1 : 0.7767\n",
      "Ignoring the Others class, Micro TP : 2369, FP : 202, FN : 236\n",
      "Accuracy : 0.9027, Micro Precision : 0.9214, Micro Recall : 0.9094, Micro F1 : 0.9154\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Simeon\\Anaconda3\\envs\\First\\lib\\site-packages\\lightgbm\\basic.py:452: UserWarning: Converting data to scipy sparse matrix.\n",
      "  warnings.warn('Converting data to scipy sparse matrix.')\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.9027223230490018, 0.9214313496693893, 0.909404990403071, 0.9153786707882534)"
      ]
     },
     "execution_count": 257,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lightGbmTrainDevKitScore(hstack([full_conv_CVect_lem, train_feats]), y,\n",
    "                         hstack([full_conv_CVect_lem_dev, dev_feats]), y_dev, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Choose top feats from CountVector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,\n",
       "        importance_type='split', learning_rate=0.1, max_depth=-1,\n",
       "        min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,\n",
       "        n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,\n",
       "        random_state=None, reg_alpha=0.0, reg_lambda=0.0, silent=True,\n",
       "        subsample=1.0, subsample_for_bin=200000, subsample_freq=0)"
      ]
     },
     "execution_count": 259,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = lightgbm.LGBMClassifier()\n",
    "clf.fit(full_conv_CVect.astype(np.float64), y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [],
   "source": [
    "feat_import = dict(zip(np.arange(full_conv_CVect.shape[1]), clf.feature_importances_))\n",
    "feat_import = sorted(feat_import.items(), key=lambda value: value[1], reverse=True)\n",
    "feat_import = {v[0]:v[1] for v in feat_import if v[1]>0}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "810"
      ]
     },
     "execution_count": 261,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(feat_import)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_most_import = full_conv_CVect.tocsc()[:,[*feat_import]]\n",
    "features_most_import_dev = full_conv_CVect_dev.tocsc()[:,[*feat_import]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training score \n",
      "\n",
      "True Positives per class :  [ 4398.  2720. 13569.  3408.]\n",
      "False Positives per class :  [ 707.  786. 3935.  637.]\n",
      "False Negatives per class :  [1108. 1523. 1379. 2055.]\n",
      "Class happy : Precision : 0.776, Recall : 0.641, F1 : 0.702\n",
      "Class sad : Precision : 0.775, Recall : 0.908, F1 : 0.836\n",
      "Class angry : Precision : 0.843, Recall : 0.624, F1 : 0.717\n",
      "Ignoring the Others class, Macro Precision : 0.7978, Macro Recall : 0.7242, Macro F1 : 0.7592\n",
      "Ignoring the Others class, Micro TP : 19697, FP : 5358, FN : 4957\n",
      "Accuracy : 0.7989, Micro Precision : 0.7862, Micro Recall : 0.7989, Micro F1 : 0.7925\n",
      "\n",
      "Dev score \n",
      "\n",
      "True Positives per class :  [ 122.   74. 2067.   76.]\n",
      "False Positives per class :  [100. 117. 122.  77.]\n",
      "False Negatives per class :  [ 28.  68. 271.  49.]\n",
      "Class happy : Precision : 0.387, Recall : 0.521, F1 : 0.444\n",
      "Class sad : Precision : 0.944, Recall : 0.884, F1 : 0.913\n",
      "Class angry : Precision : 0.497, Recall : 0.608, F1 : 0.547\n",
      "Ignoring the Others class, Macro Precision : 0.6095, Macro Recall : 0.6711, Macro F1 : 0.6388\n",
      "Ignoring the Others class, Micro TP : 2217, FP : 316, FN : 388\n",
      "Accuracy : 0.8490, Micro Precision : 0.8752, Micro Recall : 0.8511, Micro F1 : 0.8630\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.8490018148820326, 0.875246742992499, 0.8510556621880998, 0.8629817049435579)"
      ]
     },
     "execution_count": 268,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lightGbmTrainDevKitScore(features_most_import.astype(np.float64), y,\n",
    "                         features_most_import_dev.astype(np.float64), y_dev, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training score \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Simeon\\Anaconda3\\envs\\First\\lib\\site-packages\\lightgbm\\basic.py:452: UserWarning: Converting data to scipy sparse matrix.\n",
      "  warnings.warn('Converting data to scipy sparse matrix.')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True Positives per class :  [ 4744.  3722. 14052.  4665.]\n",
      "False Positives per class :  [ 449.  466. 1654.  408.]\n",
      "False Negatives per class :  [762. 521. 896. 798.]\n",
      "Class happy : Precision : 0.889, Recall : 0.877, F1 : 0.883\n",
      "Class sad : Precision : 0.895, Recall : 0.940, F1 : 0.917\n",
      "Class angry : Precision : 0.920, Recall : 0.854, F1 : 0.886\n",
      "Ignoring the Others class, Macro Precision : 0.9010, Macro Recall : 0.8904, Macro F1 : 0.8957\n",
      "Ignoring the Others class, Micro TP : 22439, FP : 2528, FN : 2215\n",
      "Accuracy : 0.9013, Micro Precision : 0.8987, Micro Recall : 0.9102, Micro F1 : 0.9044\n",
      "\n",
      "Dev score \n",
      "\n",
      "True Positives per class :  [ 122.  108. 2159.   93.]\n",
      "False Positives per class :  [77. 77. 81. 38.]\n",
      "False Negatives per class :  [ 28.  34. 179.  32.]\n",
      "Class happy : Precision : 0.584, Recall : 0.761, F1 : 0.661\n",
      "Class sad : Precision : 0.964, Recall : 0.923, F1 : 0.943\n",
      "Class angry : Precision : 0.710, Recall : 0.744, F1 : 0.727\n",
      "Ignoring the Others class, Macro Precision : 0.7525, Macro Recall : 0.8093, Macro F1 : 0.7799\n",
      "Ignoring the Others class, Micro TP : 2360, FP : 196, FN : 245\n",
      "Accuracy : 0.9009, Micro Precision : 0.9233, Micro Recall : 0.9060, Micro F1 : 0.9146\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Simeon\\Anaconda3\\envs\\First\\lib\\site-packages\\lightgbm\\basic.py:452: UserWarning: Converting data to scipy sparse matrix.\n",
      "  warnings.warn('Converting data to scipy sparse matrix.')\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.9009074410163339,\n",
       " 0.9233176838810642,\n",
       " 0.9059500959692899,\n",
       " 0.9145514435186979)"
      ]
     },
     "execution_count": 269,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lightGbmTrainDevKitScore(hstack([features_most_import, train_feats]), y,\n",
    "                         hstack([features_most_import_dev, dev_feats]), y_dev, True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary so far "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dev scores for different set of features:\n",
    "\n",
    "Manual features Only ==================\n",
    "Class happy : Precision : 0.534, Recall : 0.613, F1 : 0.570\n",
    "Class sad : Precision : 0.949, Recall : 0.896, F1 : 0.922\n",
    "Class angry : Precision : 0.555, Recall : 0.608, F1 : 0.580\n",
    "Ignoring the Others class, Macro Precision : 0.6792, Macro Recall : 0.7056, Macro F1 : 0.6922\n",
    "Ignoring the Others class, Micro TP : 2258, FP : 249, FN : 347\n",
    "Accuracy : 0.8577, Micro Precision : 0.9007, Micro Recall : 0.8668, Micro F1 : 0.8834\n",
    "\n",
    "nrc_lexicon_feats and dm_lexicon_feats only ===========\n",
    "\n",
    "Class happy : Precision : 0.338, Recall : 0.380, F1 : 0.358\n",
    "Class sad : Precision : 0.927, Recall : 0.877, F1 : 0.901\n",
    "Class angry : Precision : 0.412, Recall : 0.528, F1 : 0.463\n",
    "Ignoring the Others class, Macro Precision : 0.5591, Macro Recall : 0.5950, Macro F1 : 0.5765\n",
    "Ignoring the Others class, Micro TP : 2170, FP : 361, FN : 435\n",
    "Accuracy : 0.8218, Micro Precision : 0.8574, Micro Recall : 0.8330, Micro F1 : 0.8450\n",
    "\n",
    "emojis only ==============\n",
    "\n",
    "Class happy : Precision : 0.640, Recall : 0.387, F1 : 0.482\n",
    "Class sad : Precision : 0.872, Recall : 0.980, F1 : 0.923\n",
    "Class angry : Precision : 0.556, Recall : 0.120, F1 : 0.197\n",
    "Ignoring the Others class, Macro Precision : 0.6891, Macro Recall : 0.4957, Macro F1 : 0.5766\n",
    "Ignoring the Others class, Micro TP : 2361, FP : 379, FN : 244\n",
    "Accuracy : 0.8603, Micro Precision : 0.8617, Micro Recall : 0.9063, Micro F1 : 0.8834\n",
    "\n",
    "Word Count Vectors only  ============\n",
    "\n",
    "Class happy : Precision : 0.387, Recall : 0.521, F1 : 0.444\n",
    "Class sad : Precision : 0.944, Recall : 0.884, F1 : 0.913\n",
    "Class angry : Precision : 0.497, Recall : 0.608, F1 : 0.547\n",
    "Ignoring the Others class, Macro Precision : 0.6095, Macro Recall : 0.6711, Macro F1 : 0.6388\n",
    "Ignoring the Others class, Micro TP : 2217, FP : 316, FN : 388\n",
    "Accuracy : 0.8490, Micro Precision : 0.8752, Micro Recall : 0.8511, Micro F1 : 0.8630\n",
    "\n",
    "Manual + Word Count Vect =======\n",
    "\n",
    "Class happy : Precision : 0.571, Recall : 0.761, F1 : 0.653\n",
    "Class sad : Precision : 0.964, Recall : 0.921, F1 : 0.942\n",
    "Class angry : Precision : 0.709, Recall : 0.760, F1 : 0.734\n",
    "Ignoring the Others class, Macro Precision : 0.7480, Macro Recall : 0.8138, Macro F1 : 0.7795\n",
    "Ignoring the Others class, Micro TP : 2356, FP : 201, FN : 249\n",
    "Accuracy : 0.8984, Micro Precision : 0.9214, Micro Recall : 0.9044, Micro F1 : 0.9128\n",
    "\n",
    "Word count vect LightGBM most important + Manual Feats ====\n",
    "\n",
    "Class happy : Precision : 0.584, Recall : 0.761, F1 : 0.661\n",
    "Class sad : Precision : 0.964, Recall : 0.923, F1 : 0.943\n",
    "Class angry : Precision : 0.710, Recall : 0.744, F1 : 0.727\n",
    "Ignoring the Others class, Macro Precision : 0.7525, Macro Recall : 0.8093, Macro F1 : 0.7799\n",
    "Ignoring the Others class, Micro TP : 2360, FP : 196, FN : 245\n",
    "Accuracy : 0.9009, Micro Precision : 0.9233, Micro Recall : 0.9060, Micro F1 : 0.9146\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_selection import SelectFromModel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature selection with Logistic Regeression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ctv = CountVectorizer(analyzer='word',token_pattern=r'\\w{1,}',\n",
    "            ngram_range=(1, 3), stop_words = 'english')\n",
    "full_conv = X[['turn1', 'turn2', 'turn3']].apply(lambda x: ' '.join(x), axis=1)\n",
    "full_conv_CVect = ctv.fit_transform(full_conv)\n",
    "full_conv_dev = X_dev[['turn1', 'turn2', 'turn3']].apply(lambda x: ' '.join(x), axis=1)\n",
    "full_conv_CVect_dev = ctv.transform(full_conv_dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before Selection:  (30160, 228952)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Simeon\\Anaconda3\\envs\\First\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\Simeon\\Anaconda3\\envs\\First\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:459: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After Selection: (30160, 5483)\n"
     ]
    }
   ],
   "source": [
    "print('Before Selection: ', full_conv_CVect.shape)\n",
    "lr = LogisticRegression(penalty='l1')\n",
    "lr.fit(full_conv_CVect, y)\n",
    "model = SelectFromModel(lr, prefit=True)\n",
    "X_new = model.transform(full_conv_CVect)\n",
    "X_new_dev = model.transform(full_conv_CVect_dev)\n",
    "print('After Selection:', X_new.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training score \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Simeon\\Anaconda3\\envs\\First\\lib\\site-packages\\lightgbm\\basic.py:452: UserWarning: Converting data to scipy sparse matrix.\n",
      "  warnings.warn('Converting data to scipy sparse matrix.')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True Positives per class :  [ 4739.  3719. 14061.  4680.]\n",
      "False Positives per class :  [ 437.  459. 1653.  412.]\n",
      "False Negatives per class :  [767. 524. 887. 783.]\n",
      "Class happy : Precision : 0.890, Recall : 0.877, F1 : 0.883\n",
      "Class sad : Precision : 0.895, Recall : 0.941, F1 : 0.917\n",
      "Class angry : Precision : 0.919, Recall : 0.857, F1 : 0.887\n",
      "Ignoring the Others class, Macro Precision : 0.9013, Macro Recall : 0.8913, Macro F1 : 0.8963\n",
      "Ignoring the Others class, Micro TP : 22460, FP : 2524, FN : 2194\n",
      "Accuracy : 0.9018, Micro Precision : 0.8990, Micro Recall : 0.9110, Micro F1 : 0.9050\n",
      "\n",
      "Dev score \n",
      "\n",
      "True Positives per class :  [ 120.  107. 2155.   95.]\n",
      "False Positives per class :  [79. 80. 82. 37.]\n",
      "False Negatives per class :  [ 30.  35. 183.  30.]\n",
      "Class happy : Precision : 0.572, Recall : 0.754, F1 : 0.650\n",
      "Class sad : Precision : 0.963, Recall : 0.922, F1 : 0.942\n",
      "Class angry : Precision : 0.720, Recall : 0.760, F1 : 0.739\n",
      "Ignoring the Others class, Macro Precision : 0.7517, Macro Recall : 0.8117, Macro F1 : 0.7806\n",
      "Ignoring the Others class, Micro TP : 2357, FP : 199, FN : 248\n",
      "Accuracy : 0.8991, Micro Precision : 0.9221, Micro Recall : 0.9048, Micro F1 : 0.9134\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Simeon\\Anaconda3\\envs\\First\\lib\\site-packages\\lightgbm\\basic.py:452: UserWarning: Converting data to scipy sparse matrix.\n",
      "  warnings.warn('Converting data to scipy sparse matrix.')\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.899092558983666, 0.9221439749608764, 0.9047984644913628, 0.9133888781243944)"
      ]
     },
     "execution_count": 228,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lightGbmTrainDevKitScore(hstack([X_new, train_feats]), y,\n",
    "                         hstack([X_new_dev, dev_feats]), y_dev, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before Selection:  (30160, 5859)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Simeon\\Anaconda3\\envs\\First\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\Simeon\\Anaconda3\\envs\\First\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:459: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After Selection: (30160, 3745)\n"
     ]
    }
   ],
   "source": [
    "print('Before Selection: ', hstack([X_new, all_feats]).shape)\n",
    "lr = LogisticRegression(penalty='l1')\n",
    "lr.fit(hstack([X_new, train_feats]), y)\n",
    "model = SelectFromModel(lr, prefit=True)\n",
    "X_new_2 = model.transform(hstack([X_new, train_feats]))\n",
    "X_new_2_dev = model.transform(hstack([X_new_dev, dev_feats]))\n",
    "print('After Selection:', X_new_2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training score \n",
      "\n",
      "True Positives per class :  [ 4730.  3731. 14053.  4676.]\n",
      "False Positives per class :  [ 437.  468. 1651.  414.]\n",
      "False Negatives per class :  [776. 512. 895. 787.]\n",
      "Class happy : Precision : 0.889, Recall : 0.879, F1 : 0.884\n",
      "Class sad : Precision : 0.895, Recall : 0.940, F1 : 0.917\n",
      "Class angry : Precision : 0.919, Recall : 0.856, F1 : 0.886\n",
      "Ignoring the Others class, Macro Precision : 0.9007, Macro Recall : 0.8918, Macro F1 : 0.8962\n",
      "Ignoring the Others class, Micro TP : 22460, FP : 2533, FN : 2194\n",
      "Accuracy : 0.9015, Micro Precision : 0.8987, Micro Recall : 0.9110, Micro F1 : 0.9048\n",
      "\n",
      "Dev score \n",
      "\n",
      "True Positives per class :  [ 122.  107. 2158.   92.]\n",
      "False Positives per class :  [80. 78. 83. 35.]\n",
      "False Negatives per class :  [ 28.  35. 180.  33.]\n",
      "Class happy : Precision : 0.578, Recall : 0.754, F1 : 0.654\n",
      "Class sad : Precision : 0.963, Recall : 0.923, F1 : 0.943\n",
      "Class angry : Precision : 0.724, Recall : 0.736, F1 : 0.730\n",
      "Ignoring the Others class, Macro Precision : 0.7553, Macro Recall : 0.8042, Macro F1 : 0.7789\n",
      "Ignoring the Others class, Micro TP : 2357, FP : 196, FN : 248\n",
      "Accuracy : 0.8998, Micro Precision : 0.9232, Micro Recall : 0.9048, Micro F1 : 0.9139\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.8998185117967332,\n",
       " 0.9232275754014885,\n",
       " 0.9047984644913628,\n",
       " 0.9139201240791005)"
      ]
     },
     "execution_count": 232,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lightGbmTrainDevKitScore(X_new_2, y, X_new_2_dev, y_dev, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before Selection:  (30160, 229340)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Simeon\\Anaconda3\\envs\\First\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\Simeon\\Anaconda3\\envs\\First\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:459: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After Selection: (30160, 4344)\n",
      "Training score \n",
      "\n",
      "True Positives per class :  [ 4730.  3731. 14053.  4676.]\n",
      "False Positives per class :  [ 437.  468. 1651.  414.]\n",
      "False Negatives per class :  [776. 512. 895. 787.]\n",
      "Class happy : Precision : 0.889, Recall : 0.879, F1 : 0.884\n",
      "Class sad : Precision : 0.895, Recall : 0.940, F1 : 0.917\n",
      "Class angry : Precision : 0.919, Recall : 0.856, F1 : 0.886\n",
      "Ignoring the Others class, Macro Precision : 0.9007, Macro Recall : 0.8918, Macro F1 : 0.8962\n",
      "Ignoring the Others class, Micro TP : 22460, FP : 2533, FN : 2194\n",
      "Accuracy : 0.9015, Micro Precision : 0.8987, Micro Recall : 0.9110, Micro F1 : 0.9048\n",
      "\n",
      "Dev score \n",
      "\n",
      "True Positives per class :  [ 122.  107. 2158.   92.]\n",
      "False Positives per class :  [80. 78. 83. 35.]\n",
      "False Negatives per class :  [ 28.  35. 180.  33.]\n",
      "Class happy : Precision : 0.578, Recall : 0.754, F1 : 0.654\n",
      "Class sad : Precision : 0.963, Recall : 0.923, F1 : 0.943\n",
      "Class angry : Precision : 0.724, Recall : 0.736, F1 : 0.730\n",
      "Ignoring the Others class, Macro Precision : 0.7553, Macro Recall : 0.8042, Macro F1 : 0.7789\n",
      "Ignoring the Others class, Micro TP : 2357, FP : 196, FN : 248\n",
      "Accuracy : 0.8998, Micro Precision : 0.9232, Micro Recall : 0.9048, Micro F1 : 0.9139\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.8998185117967332,\n",
       " 0.9232275754014885,\n",
       " 0.9047984644913628,\n",
       " 0.9139201240791005)"
      ]
     },
     "execution_count": 294,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('Before Selection: ', hstack([full_conv_CVect, train_feats]).shape)\n",
    "lr = LogisticRegression(penalty='l1')\n",
    "lr.fit(hstack([full_conv_CVect, train_feats]), y)\n",
    "model = SelectFromModel(lr, prefit=True)\n",
    "X_new_3 = model.transform(hstack([full_conv_CVect, train_feats]))\n",
    "X_new_dev_3 = model.transform(hstack([full_conv_CVect_dev, dev_feats]))\n",
    "print('After Selection:', X_new_3.shape)\n",
    "lightGbmTrainDevKitScore(X_new_3, y, X_new_dev_3, y_dev, True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary for Logistic Regression Feats Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dev score\n",
    " \n",
    "Feature selection from the VectorCount and then stacking with the manual feats ====\n",
    "\n",
    "Class happy : Precision : 0.572, Recall : 0.754, F1 : 0.650\n",
    "Class sad : Precision : 0.963, Recall : 0.922, F1 : 0.942\n",
    "Class angry : Precision : 0.720, Recall : 0.760, F1 : 0.739\n",
    "Ignoring the Others class, Macro Precision : 0.7517, Macro Recall : 0.8117, Macro F1 : 0.7806\n",
    "Ignoring the Others class, Micro TP : 2357, FP : 199, FN : 248\n",
    "Accuracy : 0.8991, Micro Precision : 0.9221, Micro Recall : 0.9048, Micro F1 : 0.9134\n",
    "\n",
    "Feature selection from the result from the first selection and the manual feats\n",
    "\n",
    "Class happy : Precision : 0.578, Recall : 0.754, F1 : 0.654\n",
    "Class sad : Precision : 0.963, Recall : 0.923, F1 : 0.943\n",
    "Class angry : Precision : 0.724, Recall : 0.736, F1 : 0.730\n",
    "Ignoring the Others class, Macro Precision : 0.7553, Macro Recall : 0.8042, Macro F1 : 0.7789\n",
    "Ignoring the Others class, Micro TP : 2357, FP : 196, FN : 248\n",
    "Accuracy : 0.8998, Micro Precision : 0.9232, Micro Recall : 0.9048, Micro F1 : 0.9139\n",
    "\n",
    "Feature selection from the VectorCount and the manual feats ====\n",
    " \n",
    "Class happy : Precision : 0.578, Recall : 0.754, F1 : 0.654\n",
    "Class sad : Precision : 0.963, Recall : 0.923, F1 : 0.943\n",
    "Class angry : Precision : 0.724, Recall : 0.736, F1 : 0.730\n",
    "Ignoring the Others class, Macro Precision : 0.7553, Macro Recall : 0.8042, Macro F1 : 0.7789\n",
    "Ignoring the Others class, Micro TP : 2357, FP : 196, FN : 248\n",
    "Accuracy : 0.8998, Micro Precision : 0.9232, Micro Recall : 0.9048, Micro F1 : 0.9139"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature selection with Linear Discriminant Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training score \n",
      "\n",
      "True Positives per class :  [ 5204.  3991. 14192.  5116.]\n",
      "False Positives per class :  [277. 326. 692. 362.]\n",
      "False Negatives per class :  [302. 252. 756. 347.]\n",
      "Class happy : Precision : 0.924, Recall : 0.941, F1 : 0.932\n",
      "Class sad : Precision : 0.954, Recall : 0.949, F1 : 0.951\n",
      "Class angry : Precision : 0.934, Recall : 0.936, F1 : 0.935\n",
      "Ignoring the Others class, Macro Precision : 0.9373, Macro Recall : 0.9422, Macro F1 : 0.9397\n",
      "Ignoring the Others class, Micro TP : 23299, FP : 1380, FN : 1355\n",
      "Accuracy : 0.9451, Micro Precision : 0.9441, Micro Recall : 0.9450, Micro F1 : 0.9446\n",
      "\n",
      "Dev score \n",
      "\n",
      "True Positives per class :  [ 117.   93. 1969.   85.]\n",
      "False Positives per class :  [126. 152.  87. 126.]\n",
      "False Negatives per class :  [ 33.  49. 369.  40.]\n",
      "Class happy : Precision : 0.380, Recall : 0.655, F1 : 0.481\n",
      "Class sad : Precision : 0.958, Recall : 0.842, F1 : 0.896\n",
      "Class angry : Precision : 0.403, Recall : 0.680, F1 : 0.506\n",
      "Ignoring the Others class, Macro Precision : 0.5800, Macro Recall : 0.7257, Macro F1 : 0.6447\n",
      "Ignoring the Others class, Micro TP : 2147, FP : 365, FN : 458\n",
      "Accuracy : 0.8218, Micro Precision : 0.8547, Micro Recall : 0.8242, Micro F1 : 0.8392\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.8217785843920146,\n",
       " 0.8546974522292994,\n",
       " 0.8241842610364684,\n",
       " 0.8391635724057065)"
      ]
     },
     "execution_count": 288,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sklearn_lda_2000 = LDA(n_components=2000)\n",
    "X_train_lda_2000 = sklearn_lda_2000.fit_transform(hstack([X_new, train_feats]).toarray(), y)\n",
    "X_dev_lda_2000 = sklearn_lda_2000.transform(hstack([X_new_dev, dev_feats]).toarray())\n",
    "lightGbmTrainDevKitScore(X_train_lda_2000, y, X_dev_lda_2000, y_dev, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Simeon\\Anaconda3\\envs\\First\\lib\\site-packages\\sklearn\\discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n"
     ]
    }
   ],
   "source": [
    "sklearn_lda = LDA(n_components=1000)\n",
    "X_train_lda = sklearn_lda.fit_transform(X_new_2.toarray(), y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_dev_lda = sklearn_lda.transform(X_new_2_dev.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training score \n",
      "\n",
      "True Positives per class :  [ 5112.  3944. 13977.  4981.]\n",
      "False Positives per class :  [386. 437. 864. 459.]\n",
      "False Negatives per class :  [394. 299. 971. 482.]\n",
      "Class happy : Precision : 0.900, Recall : 0.930, F1 : 0.915\n",
      "Class sad : Precision : 0.942, Recall : 0.935, F1 : 0.938\n",
      "Class angry : Precision : 0.916, Recall : 0.912, F1 : 0.914\n",
      "Ignoring the Others class, Macro Precision : 0.9192, Macro Recall : 0.9254, Macro F1 : 0.9223\n",
      "Ignoring the Others class, Micro TP : 22902, FP : 1760, FN : 1752\n",
      "Accuracy : 0.9288, Micro Precision : 0.9286, Micro Recall : 0.9289, Micro F1 : 0.9288\n",
      "\n",
      "Dev score \n",
      "\n",
      "True Positives per class :  [ 118.   99. 2008.   85.]\n",
      "False Positives per class :  [115. 145.  79. 106.]\n",
      "False Negatives per class :  [ 32.  43. 330.  40.]\n",
      "Class happy : Precision : 0.406, Recall : 0.697, F1 : 0.513\n",
      "Class sad : Precision : 0.962, Recall : 0.859, F1 : 0.908\n",
      "Class angry : Precision : 0.445, Recall : 0.680, F1 : 0.538\n",
      "Ignoring the Others class, Macro Precision : 0.6043, Macro Recall : 0.7453, Macro F1 : 0.6675\n",
      "Ignoring the Others class, Micro TP : 2192, FP : 330, FN : 413\n",
      "Accuracy : 0.8385, Micro Precision : 0.8692, Micro Recall : 0.8415, Micro F1 : 0.8551\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.838475499092559, 0.8691514670896114, 0.8414587332053742, 0.8550809440218452)"
      ]
     },
     "execution_count": 230,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lightGbmTrainDevKitScore(X_train_lda, y, X_dev_lda, y_dev, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Simeon\\Anaconda3\\envs\\First\\lib\\site-packages\\sklearn\\discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n"
     ]
    }
   ],
   "source": [
    "sklearn_lda_2 = LDA(n_components=1000)\n",
    "X_train_lda_2 = sklearn_lda_2.fit_transform(hstack([X_new, train_feats]).toarray(), y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_dev_lda_2 = sklearn_lda_2.transform(hstack([X_new_dev, dev_feats]).toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training score \n",
      "\n",
      "True Positives per class :  [ 5207.  3983. 14192.  5119.]\n",
      "False Positives per class :  [279. 332. 690. 358.]\n",
      "False Negatives per class :  [299. 260. 756. 344.]\n",
      "Class happy : Precision : 0.923, Recall : 0.939, F1 : 0.931\n",
      "Class sad : Precision : 0.954, Recall : 0.949, F1 : 0.952\n",
      "Class angry : Precision : 0.935, Recall : 0.937, F1 : 0.936\n",
      "Ignoring the Others class, Macro Precision : 0.9371, Macro Recall : 0.9417, Macro F1 : 0.9394\n",
      "Ignoring the Others class, Micro TP : 23294, FP : 1380, FN : 1360\n",
      "Accuracy : 0.9450, Micro Precision : 0.9441, Micro Recall : 0.9448, Micro F1 : 0.9445\n",
      "\n",
      "Dev score \n",
      "\n",
      "True Positives per class :  [ 116.   95. 1972.   87.]\n",
      "False Positives per class :  [121. 153.  86. 125.]\n",
      "False Negatives per class :  [ 34.  47. 366.  38.]\n",
      "Class happy : Precision : 0.383, Recall : 0.669, F1 : 0.487\n",
      "Class sad : Precision : 0.958, Recall : 0.843, F1 : 0.897\n",
      "Class angry : Precision : 0.410, Recall : 0.696, F1 : 0.516\n",
      "Ignoring the Others class, Macro Precision : 0.5839, Macro Recall : 0.7362, Macro F1 : 0.6512\n",
      "Ignoring the Others class, Micro TP : 2154, FP : 364, FN : 451\n",
      "Accuracy : 0.8240, Micro Precision : 0.8554, Micro Recall : 0.8269, Micro F1 : 0.8409\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.8239564428312159,\n",
       " 0.8554408260524226,\n",
       " 0.8268714011516315,\n",
       " 0.8409135272301387)"
      ]
     },
     "execution_count": 233,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lightGbmTrainDevKitScore(X_train_lda_2, y, X_dev_lda_2, y_dev, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Simeon\\Anaconda3\\envs\\First\\lib\\site-packages\\sklearn\\discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n"
     ]
    }
   ],
   "source": [
    "sklearn_lda_3 = LDA(n_components=400)\n",
    "X_train_lda_3 = sklearn_lda_3.fit_transform(hstack([X_new, train_feats]).toarray(), y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_dev_lda_3 = sklearn_lda_3.transform(hstack([X_new_dev, dev_feats]).toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training score \n",
      "\n",
      "True Positives per class :  [ 5207.  3983. 14192.  5119.]\n",
      "False Positives per class :  [279. 332. 690. 358.]\n",
      "False Negatives per class :  [299. 260. 756. 344.]\n",
      "Class happy : Precision : 0.923, Recall : 0.939, F1 : 0.931\n",
      "Class sad : Precision : 0.954, Recall : 0.949, F1 : 0.952\n",
      "Class angry : Precision : 0.935, Recall : 0.937, F1 : 0.936\n",
      "Ignoring the Others class, Macro Precision : 0.9371, Macro Recall : 0.9417, Macro F1 : 0.9394\n",
      "Ignoring the Others class, Micro TP : 23294, FP : 1380, FN : 1360\n",
      "Accuracy : 0.9450, Micro Precision : 0.9441, Micro Recall : 0.9448, Micro F1 : 0.9445\n",
      "\n",
      "Dev score \n",
      "\n",
      "True Positives per class :  [ 116.   95. 1972.   87.]\n",
      "False Positives per class :  [121. 153.  86. 125.]\n",
      "False Negatives per class :  [ 34.  47. 366.  38.]\n",
      "Class happy : Precision : 0.383, Recall : 0.669, F1 : 0.487\n",
      "Class sad : Precision : 0.958, Recall : 0.843, F1 : 0.897\n",
      "Class angry : Precision : 0.410, Recall : 0.696, F1 : 0.516\n",
      "Ignoring the Others class, Macro Precision : 0.5839, Macro Recall : 0.7362, Macro F1 : 0.6512\n",
      "Ignoring the Others class, Micro TP : 2154, FP : 364, FN : 451\n",
      "Accuracy : 0.8240, Micro Precision : 0.8554, Micro Recall : 0.8269, Micro F1 : 0.8409\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.8239564428312159,\n",
       " 0.8554408260524226,\n",
       " 0.8268714011516315,\n",
       " 0.8409135272301387)"
      ]
     },
     "execution_count": 234,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lightGbmTrainDevKitScore(X_train_lda_3, y, X_dev_lda_3, y_dev, True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary of feats selection with LDA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dev score:\n",
    "\n",
    "\n",
    "LDA with n_components 2000, using the features from  the first log regresion feat selection and stacking them with the manual feats\n",
    "\n",
    "Class happy : Precision : 0.380, Recall : 0.655, F1 : 0.481\n",
    "Class sad : Precision : 0.958, Recall : 0.842, F1 : 0.896\n",
    "Class angry : Precision : 0.403, Recall : 0.680, F1 : 0.506\n",
    "Ignoring the Others class, Macro Precision : 0.5800, Macro Recall : 0.7257, Macro F1 : 0.6447\n",
    "Ignoring the Others class, Micro TP : 2147, FP : 365, FN : 458\n",
    "Accuracy : 0.8218, Micro Precision : 0.8547, Micro Recall : 0.8242, Micro F1 : 0.8392\n",
    "\n",
    "LDA with n_components 1000, using the features from  the second log regresion feat selection\n",
    "\n",
    "Class happy : Precision : 0.406, Recall : 0.697, F1 : 0.513\n",
    "Class sad : Precision : 0.962, Recall : 0.859, F1 : 0.908\n",
    "Class angry : Precision : 0.445, Recall : 0.680, F1 : 0.538\n",
    "Ignoring the Others class, Macro Precision : 0.6043, Macro Recall : 0.7453, Macro F1 : 0.6675\n",
    "Ignoring the Others class, Micro TP : 2192, FP : 330, FN : 413\n",
    "Accuracy : 0.8385, Micro Precision : 0.8692, Micro Recall : 0.8415, Micro F1 : 0.8551\n",
    "\n",
    "LDA with n_components 1000, using the features from  the first log regresion feat selection and stacking them with the manual feats\n",
    "\n",
    "Class happy : Precision : 0.383, Recall : 0.669, F1 : 0.487\n",
    "Class sad : Precision : 0.958, Recall : 0.843, F1 : 0.897\n",
    "Class angry : Precision : 0.410, Recall : 0.696, F1 : 0.516\n",
    "Ignoring the Others class, Macro Precision : 0.5839, Macro Recall : 0.7362, Macro F1 : 0.6512\n",
    "Ignoring the Others class, Micro TP : 2154, FP : 364, FN : 451\n",
    "Accuracy : 0.8240, Micro Precision : 0.8554, Micro Recall : 0.8269, Micro F1 : 0.8409\n",
    "\n",
    "LDA with n_components 400, using the features from  the first log regresion feat selection and stacking them with the manual feats\n",
    "\n",
    "Class happy : Precision : 0.383, Recall : 0.669, F1 : 0.487\n",
    "Class sad : Precision : 0.958, Recall : 0.843, F1 : 0.897\n",
    "Class angry : Precision : 0.410, Recall : 0.696, F1 : 0.516\n",
    "Ignoring the Others class, Macro Precision : 0.5839, Macro Recall : 0.7362, Macro F1 : 0.6512\n",
    "Ignoring the Others class, Micro TP : 2154, FP : 364, FN : 451\n",
    "Accuracy : 0.8240, Micro Precision : 0.8554, Micro Recall : 0.8269, Micro F1 : 0.8409"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature selection with Chi-Square"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import SelectKBest, chi2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import GaussianNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training score \n",
      "\n",
      "True Positives per class :  [  684.   437. 14067.    57.]\n",
      "False Positives per class :  [1.2030e+03 3.2800e+02 1.3376e+04 8.0000e+00]\n",
      "False Negatives per class :  [4822. 3806.  881. 5406.]\n",
      "Class happy : Precision : 0.571, Recall : 0.103, F1 : 0.175\n",
      "Class sad : Precision : 0.513, Recall : 0.941, F1 : 0.664\n",
      "Class angry : Precision : 0.877, Recall : 0.010, F1 : 0.021\n",
      "Ignoring the Others class, Macro Precision : 0.6536, Macro Recall : 0.3515, Macro F1 : 0.4571\n",
      "Ignoring the Others class, Micro TP : 14561, FP : 13712, FN : 10093\n",
      "Accuracy : 0.5055, Micro Precision : 0.5150, Micro Recall : 0.5906, Micro F1 : 0.5502\n",
      "\n",
      "Dev score \n",
      "\n",
      "True Positives per class :  [1.700e+01 1.300e+01 2.172e+03 2.000e+00]\n",
      "False Positives per class :  [149.  40. 362.   0.]\n",
      "False Negatives per class :  [133. 129. 166. 123.]\n",
      "Class happy : Precision : 0.245, Recall : 0.092, F1 : 0.133\n",
      "Class sad : Precision : 0.857, Recall : 0.929, F1 : 0.892\n",
      "Class angry : Precision : 1.000, Recall : 0.016, F1 : 0.031\n",
      "Ignoring the Others class, Macro Precision : 0.7008, Macro Recall : 0.3455, Macro F1 : 0.4628\n",
      "Ignoring the Others class, Micro TP : 2187, FP : 402, FN : 418\n",
      "Accuracy : 0.8000, Micro Precision : 0.8447, Micro Recall : 0.8395, Micro F1 : 0.8421\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.8, 0.8447276940903824, 0.8395393474088292, 0.8421255294570659)"
      ]
     },
     "execution_count": 219,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "selector = SelectKBest(chi2, k=3000)\n",
    "X_new = selector.fit_transform(full_conv_CVect, y)\n",
    "X_new_dev = selector.transform(full_conv_CVect_dev)\n",
    "trainDevKitScore(GaussianNB(),\n",
    "                 hstack([X_new, train_feats]).toarray(), y,\n",
    "                 hstack([X_new_dev, dev_feats]).toarray(), y_dev, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training score \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Simeon\\Anaconda3\\envs\\First\\lib\\site-packages\\lightgbm\\basic.py:452: UserWarning: Converting data to scipy sparse matrix.\n",
      "  warnings.warn('Converting data to scipy sparse matrix.')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True Positives per class :  [ 4743.  3727. 14060.  4673.]\n",
      "False Positives per class :  [ 429.  464. 1656.  408.]\n",
      "False Negatives per class :  [763. 516. 888. 790.]\n",
      "Class happy : Precision : 0.889, Recall : 0.878, F1 : 0.884\n",
      "Class sad : Precision : 0.895, Recall : 0.941, F1 : 0.917\n",
      "Class angry : Precision : 0.920, Recall : 0.855, F1 : 0.886\n",
      "Ignoring the Others class, Macro Precision : 0.9012, Macro Recall : 0.8915, Macro F1 : 0.8963\n",
      "Ignoring the Others class, Micro TP : 22460, FP : 2528, FN : 2194\n",
      "Accuracy : 0.9020, Micro Precision : 0.8988, Micro Recall : 0.9110, Micro F1 : 0.9049\n",
      "\n",
      "Dev score \n",
      "\n",
      "True Positives per class :  [ 122.  107. 2159.   93.]\n",
      "False Positives per class :  [76. 81. 81. 36.]\n",
      "False Negatives per class :  [ 28.  35. 179.  32.]\n",
      "Class happy : Precision : 0.569, Recall : 0.754, F1 : 0.648\n",
      "Class sad : Precision : 0.964, Recall : 0.923, F1 : 0.943\n",
      "Class angry : Precision : 0.721, Recall : 0.744, F1 : 0.732\n",
      "Ignoring the Others class, Macro Precision : 0.7513, Macro Recall : 0.8070, Macro F1 : 0.7782\n",
      "Ignoring the Others class, Micro TP : 2359, FP : 198, FN : 246\n",
      "Accuracy : 0.9005, Micro Precision : 0.9226, Micro Recall : 0.9056, Micro F1 : 0.9140\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Simeon\\Anaconda3\\envs\\First\\lib\\site-packages\\lightgbm\\basic.py:452: UserWarning: Converting data to scipy sparse matrix.\n",
      "  warnings.warn('Converting data to scipy sparse matrix.')\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.9005444646098003,\n",
       " 0.9225655064528745,\n",
       " 0.9055662188099808,\n",
       " 0.9139868268113134)"
      ]
     },
     "execution_count": 224,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lightGbmTrainDevKitScore(hstack([X_new, train_feats]), y, hstack([X_new_dev, dev_feats]), y_dev, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {},
   "outputs": [],
   "source": [
    "selector = SelectKBest(chi2, k=1000)\n",
    "X_new = selector.fit_transform(full_conv_CVect, y)\n",
    "X_new_dev = selector.transform(full_conv_CVect_dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training score \n",
      "\n",
      "True Positives per class :  [  708.   462. 14027.    59.]\n",
      "False Positives per class :  [1.2450e+03 3.5700e+02 1.3294e+04 8.0000e+00]\n",
      "False Negatives per class :  [4798. 3781.  921. 5404.]\n",
      "Class happy : Precision : 0.564, Recall : 0.109, F1 : 0.183\n",
      "Class sad : Precision : 0.513, Recall : 0.938, F1 : 0.664\n",
      "Class angry : Precision : 0.881, Recall : 0.011, F1 : 0.021\n",
      "Ignoring the Others class, Macro Precision : 0.6527, Macro Recall : 0.3527, Macro F1 : 0.4579\n",
      "Ignoring the Others class, Micro TP : 14548, FP : 13659, FN : 10106\n",
      "Accuracy : 0.5058, Micro Precision : 0.5158, Micro Recall : 0.5901, Micro F1 : 0.5504\n",
      "\n",
      "Dev score \n",
      "\n",
      "True Positives per class :  [1.80e+01 1.30e+01 2.17e+03 2.00e+00]\n",
      "False Positives per class :  [151.  40. 361.   0.]\n",
      "False Negatives per class :  [132. 129. 168. 123.]\n",
      "Class happy : Precision : 0.245, Recall : 0.092, F1 : 0.133\n",
      "Class sad : Precision : 0.857, Recall : 0.928, F1 : 0.891\n",
      "Class angry : Precision : 1.000, Recall : 0.016, F1 : 0.031\n",
      "Ignoring the Others class, Macro Precision : 0.7009, Macro Recall : 0.3452, Macro F1 : 0.4626\n",
      "Ignoring the Others class, Micro TP : 2185, FP : 401, FN : 420\n",
      "Accuracy : 0.7996, Micro Precision : 0.8449, Micro Recall : 0.8388, Micro F1 : 0.8418\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.7996370235934664,\n",
       " 0.8449342614075793,\n",
       " 0.8387715930902111,\n",
       " 0.8418416490078983)"
      ]
     },
     "execution_count": 218,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainDevKitScore(GaussianNB(),\n",
    "                 hstack([X_new, train_feats]).toarray(), y,\n",
    "                 hstack([X_new_dev, dev_feats]).toarray(), y_dev, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training score \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Simeon\\Anaconda3\\envs\\First\\lib\\site-packages\\lightgbm\\basic.py:452: UserWarning: Converting data to scipy sparse matrix.\n",
      "  warnings.warn('Converting data to scipy sparse matrix.')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True Positives per class :  [ 4735.  3728. 14071.  4667.]\n",
      "False Positives per class :  [ 437.  464. 1652.  406.]\n",
      "False Negatives per class :  [771. 515. 877. 796.]\n",
      "Class happy : Precision : 0.889, Recall : 0.879, F1 : 0.884\n",
      "Class sad : Precision : 0.895, Recall : 0.941, F1 : 0.918\n",
      "Class angry : Precision : 0.920, Recall : 0.854, F1 : 0.886\n",
      "Ignoring the Others class, Macro Precision : 0.9014, Macro Recall : 0.8914, Macro F1 : 0.8964\n",
      "Ignoring the Others class, Micro TP : 22466, FP : 2522, FN : 2188\n",
      "Accuracy : 0.9019, Micro Precision : 0.8991, Micro Recall : 0.9113, Micro F1 : 0.9051\n",
      "\n",
      "Dev score \n",
      "\n",
      "True Positives per class :  [ 122.  108. 2161.   94.]\n",
      "False Positives per class :  [77. 78. 81. 34.]\n",
      "False Negatives per class :  [ 28.  34. 177.  31.]\n",
      "Class happy : Precision : 0.581, Recall : 0.761, F1 : 0.659\n",
      "Class sad : Precision : 0.964, Recall : 0.924, F1 : 0.944\n",
      "Class angry : Precision : 0.734, Recall : 0.752, F1 : 0.743\n",
      "Ignoring the Others class, Macro Precision : 0.7596, Macro Recall : 0.8123, Macro F1 : 0.7851\n",
      "Ignoring the Others class, Micro TP : 2363, FP : 193, FN : 242\n",
      "Accuracy : 0.9020, Micro Precision : 0.9245, Micro Recall : 0.9071, Micro F1 : 0.9157\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Simeon\\Anaconda3\\envs\\First\\lib\\site-packages\\lightgbm\\basic.py:452: UserWarning: Converting data to scipy sparse matrix.\n",
      "  warnings.warn('Converting data to scipy sparse matrix.')\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.9019963702359347,\n",
       " 0.9244913928012519,\n",
       " 0.9071017274472168,\n",
       " 0.9157140089130014)"
      ]
     },
     "execution_count": 279,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lightGbmTrainDevKitScore(hstack([X_new, train_feats]), y, hstack([X_new_dev, dev_feats]), y_dev, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training score \n",
      "\n",
      "True Positives per class :  [ 4718.  3702. 14025.  4655.]\n",
      "False Positives per class :  [ 449.  466. 1708.  437.]\n",
      "False Negatives per class :  [788. 541. 923. 808.]\n",
      "Class happy : Precision : 0.888, Recall : 0.872, F1 : 0.880\n",
      "Class sad : Precision : 0.891, Recall : 0.938, F1 : 0.914\n",
      "Class angry : Precision : 0.914, Recall : 0.852, F1 : 0.882\n",
      "Ignoring the Others class, Macro Precision : 0.8979, Macro Recall : 0.8876, Macro F1 : 0.8927\n",
      "Ignoring the Others class, Micro TP : 22382, FP : 2611, FN : 2272\n",
      "Accuracy : 0.8985, Micro Precision : 0.8955, Micro Recall : 0.9078, Micro F1 : 0.9016\n",
      "\n",
      "Dev score \n",
      "\n",
      "True Positives per class :  [ 122.  106. 2156.   93.]\n",
      "False Positives per class :  [81. 76. 83. 38.]\n",
      "False Negatives per class :  [ 28.  36. 182.  32.]\n",
      "Class happy : Precision : 0.582, Recall : 0.746, F1 : 0.654\n",
      "Class sad : Precision : 0.963, Recall : 0.922, F1 : 0.942\n",
      "Class angry : Precision : 0.710, Recall : 0.744, F1 : 0.727\n",
      "Ignoring the Others class, Macro Precision : 0.7518, Macro Recall : 0.8042, Macro F1 : 0.7771\n",
      "Ignoring the Others class, Micro TP : 2355, FP : 197, FN : 250\n",
      "Accuracy : 0.8991, Micro Precision : 0.9228, Micro Recall : 0.9040, Micro F1 : 0.9133\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.899092558983666, 0.9228056426332288, 0.9040307101727447, 0.9133216986620128)"
      ]
     },
     "execution_count": 280,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "selector = SelectKBest(chi2, k=500)\n",
    "X_new = selector.fit_transform(hstack([full_conv_CVect, train_feats]), y)\n",
    "X_new_dev = selector.transform(hstack([full_conv_CVect_dev, dev_feats]))\n",
    "lightGbmTrainDevKitScore(X_new, y, X_new_dev, y_dev, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training score \n",
      "\n",
      "True Positives per class :  [  715.   463. 14012.    53.]\n",
      "False Positives per class :  [1.2720e+03 3.5900e+02 1.3282e+04 4.0000e+00]\n",
      "False Negatives per class :  [4791. 3780.  936. 5410.]\n",
      "Class happy : Precision : 0.563, Recall : 0.109, F1 : 0.183\n",
      "Class sad : Precision : 0.513, Recall : 0.937, F1 : 0.663\n",
      "Class angry : Precision : 0.930, Recall : 0.010, F1 : 0.019\n",
      "Ignoring the Others class, Macro Precision : 0.6688, Macro Recall : 0.3521, Macro F1 : 0.4613\n",
      "Ignoring the Others class, Micro TP : 14528, FP : 13645, FN : 10126\n",
      "Accuracy : 0.5054, Micro Precision : 0.5157, Micro Recall : 0.5893, Micro F1 : 0.5500\n",
      "\n",
      "Dev score \n",
      "\n",
      "True Positives per class :  [1.900e+01 1.400e+01 2.171e+03 2.000e+00]\n",
      "False Positives per class :  [152.  38. 359.   0.]\n",
      "False Negatives per class :  [131. 128. 167. 123.]\n",
      "Class happy : Precision : 0.269, Recall : 0.099, F1 : 0.144\n",
      "Class sad : Precision : 0.858, Recall : 0.929, F1 : 0.892\n",
      "Class angry : Precision : 1.000, Recall : 0.016, F1 : 0.031\n",
      "Ignoring the Others class, Macro Precision : 0.7091, Macro Recall : 0.3477, Macro F1 : 0.4666\n",
      "Ignoring the Others class, Micro TP : 2187, FP : 397, FN : 418\n",
      "Accuracy : 0.8007, Micro Precision : 0.8464, Micro Recall : 0.8395, Micro F1 : 0.8429\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.8007259528130671,\n",
       " 0.8463622291021672,\n",
       " 0.8395393474088292,\n",
       " 0.8429369820774716)"
      ]
     },
     "execution_count": 211,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainDevKitScore(GaussianNB(),X_new.toarray(), y, X_new_dev.toarray(), y_dev, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training score \n",
      "\n",
      "True Positives per class :  [ 4699.  3689. 14025.  4635.]\n",
      "False Positives per class :  [ 461.  462. 1746.  443.]\n",
      "False Negatives per class :  [807. 554. 923. 828.]\n",
      "Class happy : Precision : 0.889, Recall : 0.869, F1 : 0.879\n",
      "Class sad : Precision : 0.889, Recall : 0.938, F1 : 0.913\n",
      "Class angry : Precision : 0.913, Recall : 0.848, F1 : 0.879\n",
      "Ignoring the Others class, Macro Precision : 0.8969, Macro Recall : 0.8854, Macro F1 : 0.8911\n",
      "Ignoring the Others class, Micro TP : 22349, FP : 2651, FN : 2305\n",
      "Accuracy : 0.8968, Micro Precision : 0.8940, Micro Recall : 0.9065, Micro F1 : 0.9002\n",
      "\n",
      "Dev score \n",
      "\n",
      "True Positives per class :  [ 122.  108. 2161.   94.]\n",
      "False Positives per class :  [74. 81. 81. 34.]\n",
      "False Negatives per class :  [ 28.  34. 177.  31.]\n",
      "Class happy : Precision : 0.571, Recall : 0.761, F1 : 0.653\n",
      "Class sad : Precision : 0.964, Recall : 0.924, F1 : 0.944\n",
      "Class angry : Precision : 0.734, Recall : 0.752, F1 : 0.743\n",
      "Ignoring the Others class, Macro Precision : 0.7566, Macro Recall : 0.8123, Macro F1 : 0.7834\n",
      "Ignoring the Others class, Micro TP : 2363, FP : 196, FN : 242\n",
      "Accuracy : 0.9020, Micro Precision : 0.9234, Micro Recall : 0.9071, Micro F1 : 0.9152\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.9019963702359347,\n",
       " 0.9234075810863619,\n",
       " 0.9071017274472168,\n",
       " 0.9151820294345467)"
      ]
     },
     "execution_count": 281,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "selector = SelectKBest(chi2, k=400)\n",
    "X_new = selector.fit_transform(hstack([full_conv_CVect, train_feats]), y)\n",
    "X_new_dev = selector.transform(hstack([full_conv_CVect_dev, dev_feats]))\n",
    "lightGbmTrainDevKitScore(X_new, y, X_new_dev, y_dev, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training score \n",
      "\n",
      "True Positives per class :  [ 4670.  3671. 14011.  4604.]\n",
      "False Positives per class :  [ 467.  486. 1819.  432.]\n",
      "False Negatives per class :  [836. 572. 937. 859.]\n",
      "Class happy : Precision : 0.883, Recall : 0.865, F1 : 0.874\n",
      "Class sad : Precision : 0.885, Recall : 0.937, F1 : 0.910\n",
      "Class angry : Precision : 0.914, Recall : 0.843, F1 : 0.877\n",
      "Ignoring the Others class, Macro Precision : 0.8941, Macro Recall : 0.8818, Macro F1 : 0.8879\n",
      "Ignoring the Others class, Micro TP : 22286, FP : 2737, FN : 2368\n",
      "Accuracy : 0.8938, Micro Precision : 0.8906, Micro Recall : 0.9040, Micro F1 : 0.8972\n",
      "\n",
      "Dev score \n",
      "\n",
      "True Positives per class :  [ 121.  107. 2154.   93.]\n",
      "False Positives per class :  [77. 81. 87. 35.]\n",
      "False Negatives per class :  [ 29.  35. 184.  32.]\n",
      "Class happy : Precision : 0.569, Recall : 0.754, F1 : 0.648\n",
      "Class sad : Precision : 0.961, Recall : 0.921, F1 : 0.941\n",
      "Class angry : Precision : 0.727, Recall : 0.744, F1 : 0.735\n",
      "Ignoring the Others class, Macro Precision : 0.7523, Macro Recall : 0.8063, Macro F1 : 0.7784\n",
      "Ignoring the Others class, Micro TP : 2354, FP : 203, FN : 251\n",
      "Accuracy : 0.8984, Micro Precision : 0.9206, Micro Recall : 0.9036, Micro F1 : 0.9120\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.8983666061705989,\n",
       " 0.9206100899491592,\n",
       " 0.9036468330134357,\n",
       " 0.9120495931809376)"
      ]
     },
     "execution_count": 282,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "selector = SelectKBest(chi2, k=300)\n",
    "X_new = selector.fit_transform(hstack([full_conv_CVect, train_feats]), y)\n",
    "X_new_dev = selector.transform(hstack([full_conv_CVect_dev, dev_feats]))\n",
    "lightGbmTrainDevKitScore(X_new, y, X_new_dev, y_dev, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training score \n",
      "\n",
      "True Positives per class :  [ 4618.  3623. 13970.  4519.]\n",
      "False Positives per class :  [ 490.  501. 1981.  458.]\n",
      "False Negatives per class :  [888. 620. 978. 944.]\n",
      "Class happy : Precision : 0.879, Recall : 0.854, F1 : 0.866\n",
      "Class sad : Precision : 0.876, Recall : 0.935, F1 : 0.904\n",
      "Class angry : Precision : 0.908, Recall : 0.827, F1 : 0.866\n",
      "Ignoring the Others class, Macro Precision : 0.8874, Macro Recall : 0.8719, Macro F1 : 0.8796\n",
      "Ignoring the Others class, Micro TP : 22112, FP : 2940, FN : 2542\n",
      "Accuracy : 0.8863, Micro Precision : 0.8826, Micro Recall : 0.8969, Micro F1 : 0.8897\n",
      "\n",
      "Dev score \n",
      "\n",
      "True Positives per class :  [ 118.  108. 2135.   89.]\n",
      "False Positives per class :  [93. 83. 89. 40.]\n",
      "False Negatives per class :  [ 32.  34. 203.  36.]\n",
      "Class happy : Precision : 0.565, Recall : 0.761, F1 : 0.649\n",
      "Class sad : Precision : 0.960, Recall : 0.913, F1 : 0.936\n",
      "Class angry : Precision : 0.690, Recall : 0.712, F1 : 0.701\n",
      "Ignoring the Others class, Macro Precision : 0.7384, Macro Recall : 0.7952, Macro F1 : 0.7658\n",
      "Ignoring the Others class, Micro TP : 2332, FP : 212, FN : 273\n",
      "Accuracy : 0.8893, Micro Precision : 0.9167, Micro Recall : 0.8952, Micro F1 : 0.9058\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.8892921960072595,\n",
       " 0.9166666666666666,\n",
       " 0.8952015355086372,\n",
       " 0.9058069528063701)"
      ]
     },
     "execution_count": 283,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "selector = SelectKBest(chi2, k=200)\n",
    "X_new = selector.fit_transform(hstack([full_conv_CVect, train_feats]), y)\n",
    "X_new_dev = selector.transform(hstack([full_conv_CVect_dev, dev_feats]))\n",
    "lightGbmTrainDevKitScore(X_new, y, X_new_dev, y_dev, True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary for Chi_2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "k=3000, Selection from CountVector + stacking manual features\n",
    "\n",
    "Class happy : Precision : 0.569, Recall : 0.754, F1 : 0.648\n",
    "Class sad : Precision : 0.964, Recall : 0.923, F1 : 0.943\n",
    "Class angry : Precision : 0.721, Recall : 0.744, F1 : 0.732\n",
    "Ignoring the Others class, Macro Precision : 0.7513, Macro Recall : 0.8070, Macro F1 : 0.7782\n",
    "Ignoring the Others class, Micro TP : 2359, FP : 198, FN : 246\n",
    "Accuracy : 0.9005, Micro Precision : 0.9226, Micro Recall : 0.9056, Micro F1 : 0.9140\n",
    "\n",
    "k=1000, Selection from CountVector + stacking manual features\n",
    "\n",
    "Class happy : Precision : 0.581, Recall : 0.761, F1 : 0.659\n",
    "Class sad : Precision : 0.964, Recall : 0.924, F1 : 0.944\n",
    "Class angry : Precision : 0.734, Recall : 0.752, F1 : 0.743\n",
    "Ignoring the Others class, Macro Precision : 0.7596, Macro Recall : 0.8123, Macro F1 : 0.7851\n",
    "Ignoring the Others class, Micro TP : 2363, FP : 193, FN : 242\n",
    "Accuracy : 0.9020, Micro Precision : 0.9245, Micro Recall : 0.9071, Micro F1 : 0.9157\n",
    "\n",
    "k=500, Selection from CountVector and manual features\n",
    "\n",
    "Class happy : Precision : 0.582, Recall : 0.746, F1 : 0.654\n",
    "Class sad : Precision : 0.963, Recall : 0.922, F1 : 0.942\n",
    "Class angry : Precision : 0.710, Recall : 0.744, F1 : 0.727\n",
    "Ignoring the Others class, Macro Precision : 0.7518, Macro Recall : 0.8042, Macro F1 : 0.7771\n",
    "Ignoring the Others class, Micro TP : 2355, FP : 197, FN : 250\n",
    "Accuracy : 0.8991, Micro Precision : 0.9228, Micro Recall : 0.9040, Micro F1 : 0.9133\n",
    "\n",
    "k=400, Selection from CountVector and manual features\n",
    "\n",
    "Class happy : Precision : 0.571, Recall : 0.761, F1 : 0.653\n",
    "Class sad : Precision : 0.964, Recall : 0.924, F1 : 0.944\n",
    "Class angry : Precision : 0.734, Recall : 0.752, F1 : 0.743\n",
    "Ignoring the Others class, Macro Precision : 0.7566, Macro Recall : 0.8123, Macro F1 : 0.7834\n",
    "Ignoring the Others class, Micro TP : 2363, FP : 196, FN : 242\n",
    "Accuracy : 0.9020, Micro Precision : 0.9234, Micro Recall : 0.9071, Micro F1 : 0.9152\n",
    "\n",
    "k=300, Selection from CountVector and manual features\n",
    "\n",
    "Class happy : Precision : 0.569, Recall : 0.754, F1 : 0.648\n",
    "Class sad : Precision : 0.961, Recall : 0.921, F1 : 0.941\n",
    "Class angry : Precision : 0.727, Recall : 0.744, F1 : 0.735\n",
    "Ignoring the Others class, Macro Precision : 0.7523, Macro Recall : 0.8063, Macro F1 : 0.7784\n",
    "Ignoring the Others class, Micro TP : 2354, FP : 203, FN : 251\n",
    "Accuracy : 0.8984, Micro Precision : 0.9206, Micro Recall : 0.9036, Micro F1 : 0.9120\n",
    "\n",
    "k=200, Selection from CountVector and manual features\n",
    "\n",
    "Class happy : Precision : 0.565, Recall : 0.761, F1 : 0.649\n",
    "Class sad : Precision : 0.960, Recall : 0.913, F1 : 0.936\n",
    "Class angry : Precision : 0.690, Recall : 0.712, F1 : 0.701\n",
    "Ignoring the Others class, Macro Precision : 0.7384, Macro Recall : 0.7952, Macro F1 : 0.7658\n",
    "Ignoring the Others class, Micro TP : 2332, FP : 212, FN : 273\n",
    "Accuracy : 0.8893, Micro Precision : 0.9167, Micro Recall : 0.8952, Micro F1 : 0.9058\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final Summary: Best results for each selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LightGBM feats selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Class happy : Precision : 0.584, Recall : 0.761, F1 : 0.661\n",
    "Class sad : Precision : 0.964, Recall : 0.923, F1 : 0.943\n",
    "Class angry : Precision : 0.710, Recall : 0.744, F1 : 0.727\n",
    "Ignoring the Others class, Macro Precision : 0.7525, Macro Recall : 0.8093, Macro F1 : 0.7799\n",
    "Ignoring the Others class, Micro TP : 2360, FP : 196, FN : 245\n",
    "Accuracy : 0.9009, Micro Precision : 0.9233, Micro Recall : 0.9060, Micro F1 : 0.9146"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regresion feats selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dev score\n",
    " \n",
    "Feature selection from the VectorCount and then stacking with the manual feats ====\n",
    "\n",
    "Class happy : Precision : 0.572, Recall : 0.754, F1 : 0.650\n",
    "Class sad : Precision : 0.963, Recall : 0.922, F1 : 0.942\n",
    "Class angry : Precision : 0.720, Recall : 0.760, F1 : 0.739\n",
    "Ignoring the Others class, Macro Precision : 0.7517, Macro Recall : 0.8117, Macro F1 : 0.7806\n",
    "Ignoring the Others class, Micro TP : 2357, FP : 199, FN : 248\n",
    "Accuracy : 0.8991, Micro Precision : 0.9221, Micro Recall : 0.9048, Micro F1 : 0.9134\n",
    "\n",
    "Feature selection from the result from the first selection and the manual feats\n",
    "\n",
    "Class happy : Precision : 0.578, Recall : 0.754, F1 : 0.654\n",
    "Class sad : Precision : 0.963, Recall : 0.923, F1 : 0.943\n",
    "Class angry : Precision : 0.724, Recall : 0.736, F1 : 0.730\n",
    "Ignoring the Others class, Macro Precision : 0.7553, Macro Recall : 0.8042, Macro F1 : 0.7789\n",
    "Ignoring the Others class, Micro TP : 2357, FP : 196, FN : 248\n",
    "Accuracy : 0.8998, Micro Precision : 0.9232, Micro Recall : 0.9048, Micro F1 : 0.9139"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LDA feats selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LDA with n_components 1000, using the features from  the second log regresion feat selection\n",
    "\n",
    "Class happy : Precision : 0.406, Recall : 0.697, F1 : 0.513\n",
    "Class sad : Precision : 0.962, Recall : 0.859, F1 : 0.908\n",
    "Class angry : Precision : 0.445, Recall : 0.680, F1 : 0.538\n",
    "Ignoring the Others class, Macro Precision : 0.6043, Macro Recall : 0.7453, Macro F1 : 0.6675\n",
    "Ignoring the Others class, Micro TP : 2192, FP : 330, FN : 413\n",
    "Accuracy : 0.8385, Micro Precision : 0.8692, Micro Recall : 0.8415, Micro F1 : 0.8551"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chi_2 feats selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "k=1000, Selection from CountVector + stacking manual features\n",
    "\n",
    "Class happy : Precision : 0.581, Recall : 0.761, F1 : 0.659\n",
    "Class sad : Precision : 0.964, Recall : 0.924, F1 : 0.944\n",
    "Class angry : Precision : 0.734, Recall : 0.752, F1 : 0.743\n",
    "Ignoring the Others class, Macro Precision : 0.7596, Macro Recall : 0.8123, Macro F1 : 0.7851\n",
    "Ignoring the Others class, Micro TP : 2363, FP : 193, FN : 242\n",
    "Accuracy : 0.9020, Micro Precision : 0.9245, Micro Recall : 0.9071, Micro F1 : 0.9157"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Saving Some of the selected features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = pd.read_csv('D:\\\\Machine Learning\\\\Datasets\\\\EmoContext\\\\testwithoutlabels.txt',\n",
    "                        sep='\\t', index_col='id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ctv = CountVectorizer(analyzer='word',token_pattern=r'\\w{1,}',\n",
    "            ngram_range=(1, 3), stop_words = 'english')\n",
    "full_conv = X[['turn1', 'turn2', 'turn3']].apply(lambda x: ' '.join(x), axis=1)\n",
    "full_conv_CVect = ctv.fit_transform(full_conv)\n",
    "full_conv_dev = X_dev[['turn1', 'turn2', 'turn3']].apply(lambda x: ' '.join(x), axis=1)\n",
    "full_conv_CVect_dev = ctv.transform(full_conv_dev)\n",
    "full_conv_test = X_test[['turn1', 'turn2', 'turn3']].apply(lambda x: ' '.join(x), axis=1)\n",
    "full_conv_CVect_test = ctv.transform(full_conv_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before Selection:  (30160, 229340)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Simeon\\Anaconda3\\envs\\First\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\Simeon\\Anaconda3\\envs\\First\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:459: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After Selection: (30160, 4340)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print('Before Selection: ', hstack([full_conv_CVect, train_feats]).shape)\n",
    "lr = LogisticRegression(penalty='l1')\n",
    "lr.fit(hstack([full_conv_CVect, train_feats]), y)\n",
    "model = SelectFromModel(lr, prefit=True)\n",
    "X_log_select = model.transform(hstack([full_conv_CVect, train_feats]))\n",
    "X_log_select_dev = model.transform(hstack([full_conv_CVect_dev, dev_feats]))\n",
    "X_log_select_test = model.transform(hstack([full_conv_CVect_test, test_feats]))\n",
    "print('After Selection:', X_log_select.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_path = 'D:\\\\Machine Learning\\\\Datasets\\\\EmoContext\\\\Features\\\\FeatSelection\\\\' \n",
    "sparse.save_npz(features_path + r'\\log_reg\\train.npz', X_log_select)\n",
    "sparse.save_npz(features_path + r'\\log_reg\\dev.npz', X_log_select_dev)\n",
    "sparse.save_npz(features_path + r'\\log_reg\\test.npz', X_log_select_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = lightgbm.LGBMClassifier()\n",
    "clf.fit(full_conv_CVect.astype(np.float64), y)\n",
    "feat_import = dict(zip(np.arange(full_conv_CVect.shape[1]), clf.feature_importances_))\n",
    "feat_import = sorted(feat_import.items(), key=lambda value: value[1], reverse=True)\n",
    "feat_import = {v[0]:v[1] for v in feat_import if v[1]>0}\n",
    "features_most_import = full_conv_CVect.tocsc()[:,[*feat_import]]\n",
    "features_most_import_dev = full_conv_CVect_dev.tocsc()[:,[*feat_import]]\n",
    "features_most_import_test = full_conv_CVect_test.tocsc()[:,[*feat_import]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "metadata": {},
   "outputs": [],
   "source": [
    "sparse.save_npz(features_path + r'\\lgbm\\train.npz',\n",
    "                hstack([features_most_import, train_feats]))\n",
    "sparse.save_npz(features_path + r'\\lgbm\\dev.npz',\n",
    "                hstack([features_most_import_dev, dev_feats]))\n",
    "sparse.save_npz(features_path + r'\\lgbm\\test.npz', \n",
    "                hstack([features_most_import_test, test_feats]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "metadata": {},
   "outputs": [],
   "source": [
    "selector = SelectKBest(chi2, k=1000)\n",
    "X_chi2 = selector.fit_transform(full_conv_CVect, y)\n",
    "X_chi2_dev = selector.transform(full_conv_CVect_dev)\n",
    "X_chi2_test = selector.transform(full_conv_CVect_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "metadata": {},
   "outputs": [],
   "source": [
    "sparse.save_npz(features_path + r'\\chi2\\train.npz',\n",
    "                hstack([X_chi2, train_feats]))\n",
    "sparse.save_npz(features_path + r'\\chi2\\dev.npz',\n",
    "                hstack([X_chi2_dev, dev_feats]))\n",
    "sparse.save_npz(features_path + r'\\chi2\\test.npz', \n",
    "                hstack([X_chi2_test, test_feats]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "================================"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "================================"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = pd.read_csv('D:\\\\Machine Learning\\\\Datasets\\\\EmoContext\\\\testwithoutlabels.txt',\n",
    "                        sep='\\t', index_col='id')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_conv_test = X_test[['turn1', 'turn2', 'turn3']].apply(lambda x: ' '.join(x), axis=1)\n",
    "full_conv_CVect_test = ctv.transform(full_conv_test)\n",
    "X_new_test = selector.transform(hstack([full_conv_CVect_test, test_feats]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_conv_CVect_test = ctv.transform(full_conv_test)\n",
    "X_new_test = full_conv_CVect_test.tocsc()[:,[*feat_import]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Simeon\\Anaconda3\\envs\\First\\lib\\site-packages\\lightgbm\\basic.py:452: UserWarning: Converting data to scipy sparse matrix.\n",
      "  warnings.warn('Converting data to scipy sparse matrix.')\n"
     ]
    }
   ],
   "source": [
    "clf = lightgbm.LGBMClassifier(random_state=42)\n",
    "clf.fit(hstack([features_most_import, train_feats]), y)\n",
    "test_pred = clf.predict(hstack([X_new_test, test_feats]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_labels = le.inverse_transform(test_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['others', 'others', 'angry', ..., 'others', 'others', 'others'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 276,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {},
   "outputs": [],
   "source": [
    "solutionPath = 'D:\\\\Machine Learning\\\\Datasets\\\\EmoContext\\\\test.txt'\n",
    "testDataPath = 'D:\\\\Machine Learning\\\\Datasets\\\\EmoContext\\\\testwithoutlabels.txt'\n",
    "with io.open(solutionPath, \"w\", encoding=\"utf8\") as fout:\n",
    "        fout.write('\\t'.join([\"id\", \"turn1\", \"turn2\", \"turn3\", \"label\"]) + '\\n')        \n",
    "        with io.open(testDataPath, encoding=\"utf8\") as fin:\n",
    "            fin.readline()\n",
    "            for lineNum, line in enumerate(fin):\n",
    "                fout.write('\\t'.join(line.strip().split('\\t')[:4]) + '\\t')\n",
    "                fout.write(test_labels[lineNum] + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
